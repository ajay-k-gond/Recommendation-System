{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the chunk of dataset we had loaded in EDA notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-MhfebM0QIsKt87iDN-FNw</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-04-15 05:21:16</td>\n",
       "      <td>0</td>\n",
       "      <td>xQY8N_XvtGbearJ5X4QryQ</td>\n",
       "      <td>2</td>\n",
       "      <td>As someone who has worked with many museums, I...</td>\n",
       "      <td>5</td>\n",
       "      <td>OwjRMXRC0KyPrIlcjaXeFQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lbrU8StCq3yDfr-QMnGrmQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-12-07 03:16:52</td>\n",
       "      <td>1</td>\n",
       "      <td>UmFMZ8PyXZTY2QcwzsfQYA</td>\n",
       "      <td>1</td>\n",
       "      <td>I am actually horrified this place is still in...</td>\n",
       "      <td>1</td>\n",
       "      <td>nIJD_7ZXHq-FX8byPMOkMQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HQl28KMwrEKHqhFrrDqVNQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-05 03:18:11</td>\n",
       "      <td>0</td>\n",
       "      <td>LG2ZaYiOgpr2DK_90pYjNw</td>\n",
       "      <td>5</td>\n",
       "      <td>I love Deagan's. I do. I really do. The atmosp...</td>\n",
       "      <td>1</td>\n",
       "      <td>V34qejxNsCbcgD8C0HVk-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5JxlZaqCnk1MnbgRirs40Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2011-05-27 05:30:52</td>\n",
       "      <td>0</td>\n",
       "      <td>i6g_oA9Yf9Y31qt0wibXpw</td>\n",
       "      <td>1</td>\n",
       "      <td>Dismal, lukewarm, defrosted-tasting \"TexMex\" g...</td>\n",
       "      <td>0</td>\n",
       "      <td>ofKDkJKXSKZXu5xJNGiiBQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IS4cv902ykd8wj1TR0N3-A</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:56:57</td>\n",
       "      <td>0</td>\n",
       "      <td>6TdNDKywdbjoTkizeMce8A</td>\n",
       "      <td>4</td>\n",
       "      <td>Oh happy day, finally have a Canes near my cas...</td>\n",
       "      <td>0</td>\n",
       "      <td>UgMW8bLE0QMJDCkQ1Ax5Mg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  -MhfebM0QIsKt87iDN-FNw     0  2015-04-15 05:21:16      0   \n",
       "1  lbrU8StCq3yDfr-QMnGrmQ     0  2013-12-07 03:16:52      1   \n",
       "2  HQl28KMwrEKHqhFrrDqVNQ     0  2015-12-05 03:18:11      0   \n",
       "3  5JxlZaqCnk1MnbgRirs40Q     0  2011-05-27 05:30:52      0   \n",
       "4  IS4cv902ykd8wj1TR0N3-A     0  2017-01-14 21:56:57      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  xQY8N_XvtGbearJ5X4QryQ      2   \n",
       "1  UmFMZ8PyXZTY2QcwzsfQYA      1   \n",
       "2  LG2ZaYiOgpr2DK_90pYjNw      5   \n",
       "3  i6g_oA9Yf9Y31qt0wibXpw      1   \n",
       "4  6TdNDKywdbjoTkizeMce8A      4   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  As someone who has worked with many museums, I...       5   \n",
       "1  I am actually horrified this place is still in...       1   \n",
       "2  I love Deagan's. I do. I really do. The atmosp...       1   \n",
       "3  Dismal, lukewarm, defrosted-tasting \"TexMex\" g...       0   \n",
       "4  Oh happy day, finally have a Canes near my cas...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  OwjRMXRC0KyPrIlcjaXeFQ  \n",
       "1  nIJD_7ZXHq-FX8byPMOkMQ  \n",
       "2  V34qejxNsCbcgD8C0HVk-Q  \n",
       "3  ofKDkJKXSKZXu5xJNGiiBQ  \n",
       "4  UgMW8bLE0QMJDCkQ1Ax5Mg  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"datasets/dataset.csv\", encoding = \"ISO-8859â€“1\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Imputations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section we are doing following operations to clean the data and convert to a proper format\n",
    "#### 1. We are converting all the characters into lower case\n",
    "#### 2. We are removing the stop words beacuse stop words will not capture any contextual information from the corpus\n",
    "#### 3. Replacing some short-forms to thier proper representation.\n",
    "#### 4. below is the set of stop words from english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ajay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(text):\n",
    "    text = text.translate(string.punctuation)\n",
    "    text = text.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
    "    text = \" \".join(text)\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only taking the set of attributes we are interested in.\n",
    "### Latar on I will have to use sparse matrix size of 10K * 10K as per input. This will give memory error in the local machine so I am taking a set of 1K for traning and 500 for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = data[1000:1500]\n",
    "data = data[['business_id', 'user_id', 'stars', 'text']][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-koQoVsEGgN03TAgQd_DWA',\n",
       " 'bHXujstlLp-QuNr72Meprw',\n",
       " 'VQrHL8gzDlFz0jGbrXq3yw',\n",
       " 'jMgQQwSLjyqSvckG_DvZIw',\n",
       " '82ksBgi3GcayHuPE87zD-w',\n",
       " '_5hoVyqm1ghfXVymvZjzKQ',\n",
       " 'xUhKi3p2BRTEbTHtJz-Hhg',\n",
       " 'M6-A6F0B3kM5i94Kr0XHcw',\n",
       " 'YLyFQ9VVBoqqPIiUeDvVZw',\n",
       " '2NkVhK4-yLprEBeYqqn3xw',\n",
       " '4r2M6cFPugL7cJmc_6VYGA',\n",
       " 'BMm9-bo1Xar-Vm8VMnQ88g',\n",
       " 'jmyunODJvYT7n7LCgotAyQ',\n",
       " 'SbsUVsP2gkQhJf4L2q4kjg',\n",
       " 'H6j_KNWrrcRYYndQgAUizw',\n",
       " 'Ibl1msXxcsHrzBp8oxTr0g',\n",
       " 'Q4-IBfwUPk3uitiaho_dqw',\n",
       " 'Fds6mttIFKPsSmY8xfC_XA',\n",
       " '9-SlQK2lwcXVzk3tJU4x7g',\n",
       " 'M3ncFIlEfaSdSpoiMINwBA',\n",
       " 'ZBlSML8YMmfcxNo-zdH1_A',\n",
       " 'uBSMteq_cq9iT1MMGmXtvQ',\n",
       " 'zlQy2mGCbiYzhcbgJ60rig',\n",
       " 'X4cwQL_JZZnAUyCbOwz3pw',\n",
       " '9Jpq9Rtg6xx3nisN-FgGFg',\n",
       " '2xfTH1pK3gPtpEh4Gz_Y3w',\n",
       " 'SehV0pAEJNVJvd5_5rtTyg',\n",
       " 'EBwGWhbi49i2DBF61hVybg',\n",
       " 'yOM891kAdwni5ecnGsQcWw',\n",
       " 'dEDId4Rp2JpblIfYIoH00w',\n",
       " 'o8gCgAuBdy7OdmU7jxjW1g',\n",
       " 'DzQqwNMb1v5fZtW6urXd1w',\n",
       " 'g-_TsZNEJ_hls717NuGekw',\n",
       " 'M1mSvucWF3V1h0L98okb9g',\n",
       " 'Dln9H9qc9EbKaC-c0FCFFA',\n",
       " 'xftnxppMN_gUt5oknMu4dA',\n",
       " 'xSiaSnRfXBVZstO9LBNRaA',\n",
       " 'ibJb_abK_o19V28_MB33qA',\n",
       " 'eqav5CqqFPgqEjCTUVH_nQ',\n",
       " 'P72XL2i5H1a_pd4wiGvkdg',\n",
       " '7XXcCSOP1j6Kz2A2EZpaew',\n",
       " '_J4u2AOsuWhUmCAPnnAIiQ',\n",
       " 'VmRad5bBsorlNZbE1gyNFQ',\n",
       " 'eTqEEhDaNPby1oZXFL1eoA',\n",
       " '-WnJbMzRQVBQv5sWfZJi7A',\n",
       " 'BWFF5dX7oE5F-IB8hc36Kg',\n",
       " 'UXytoH5MrajQsxbSFliPrQ',\n",
       " 'S5oMD5z3sQ2K93UunjBBUQ',\n",
       " 'qSH7p_vKcDY02LhxHjjfMw',\n",
       " 'fOG0S2zgsbDsx94R7vMF8w',\n",
       " '-AGd-1yGEGA_6sXW7X15oA',\n",
       " 'UTkvIBuXLX75HSm-7DAmmg',\n",
       " 'kJ4F3mKCwVt-eC6qqoTysA',\n",
       " 'We3Bp7FXfnvBz4742UGsXQ',\n",
       " 'SLyyJdW0TDljb8BhLvugIg',\n",
       " '9Wn0uZZuLN6_NCEQ-o9n7A',\n",
       " 'L5JFnETi16y2gNiESXBYeA',\n",
       " 'oWkqynBXgPy9db7GijMTwQ',\n",
       " 'gtbTQDg1vmtnmgOcnbp_zg',\n",
       " 'SGoRQS17lLzewYPFEZ4yrg',\n",
       " 'SL32HFx_HMtO_tOpjkF4MA',\n",
       " '_Gxh54UrguA_Nz673I5bqQ',\n",
       " 'Qh4s4cirOrG8MdaIbH6Udw',\n",
       " 'Ro9TLRu_obvJj4B05Do-Zw',\n",
       " 'c0UJR7iu4gaAyLqb0RPjzw',\n",
       " 'C7M0oEttXPDR4efWeSXMUA',\n",
       " 'oerw70Uc1dQl8t-86fOUsg',\n",
       " 'LF6aUpdc2KUkbk4Y9SOKsA',\n",
       " '488KrW58ryVduy0Me8DRxg',\n",
       " '3QCugmCSsCOHsC3fE0NBjg',\n",
       " 'W0HE4BZjIu2l0d-0uPsmew',\n",
       " 'be4Y79c85n3Dk_TlssM08Q',\n",
       " 'mVVfJBBJ8OvhHsxncB4OWw',\n",
       " 'k6oUsuhyJ2VtsdqIiD2oRw',\n",
       " '2-2qFkXeGh4jG7MkAevdQg',\n",
       " 'A3el6ctiVe6HIZj6y74Bsw',\n",
       " 'rCWrxuRC8_pfagpchtHp6A',\n",
       " 'UtcbovfseS92JPloUWfNsg',\n",
       " 'LAIGnIbhrmwjJYovnwQ-4A',\n",
       " 'wfu1YP-mLfxZSb0zDGbXaA',\n",
       " '-xlOz4tH5iEbh2mcyoLWzA',\n",
       " 'F1Snbo0Rw3VZ2m04LMCnPg',\n",
       " 'tvateFfxBXqHMFWleOHGYA',\n",
       " '5GCRxbF59G1ae38H7SdMOQ',\n",
       " 'WMv42g9xwWDZQGcDQfIfrg',\n",
       " 'TNhrTCVgDJ0lLxM0CBqssQ',\n",
       " '7OPBkeJT4He4XEWLDfw0_g',\n",
       " 'lg1UvP-7KLtjSdxbFbt3Aw',\n",
       " 'VC08sCbP8AXjUhwl5VNhHQ',\n",
       " 'Hxjcloe2LCoG5HZratFsHg',\n",
       " 'K9nSjJPsMdRrJciaTGd6SA',\n",
       " 'vNbr4JUZtIDB34xT-1n8ew',\n",
       " 'JUeGFCsprKVUYok4oLsdEA',\n",
       " 'D5ywfFmwtJxLReqAYlDDmw',\n",
       " 'nF8BqK9o7prWfuZCwiOYkQ',\n",
       " 'LNybdWRnPgauhsJwxfGbBA',\n",
       " '5UQFZg5Vek6JHTIIhKxOiA',\n",
       " 'C0SxQ9hRe2HhTjmIrFEcaw',\n",
       " 'Y-eZiZcWFbjgFUyGNdQ7Dg',\n",
       " 'PdgpUK6fHArEXsPcKFHXOg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test_set['user_id'].unique())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(clean_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we are going to implement Collaborative Filtering method, we need to split attributes into user and restaurent dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = data[['user_id','text']]\n",
    "rest_df = data[['business_id', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From EDA notebook we came to know that there is one to many mapping with users and reviews therefore we will aggregate reviews for each users and same for restaurents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = user_df.groupby('user_id').agg({'text': ' '.join})\n",
    "rest_df = rest_df.groupby('business_id').agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1eORDLXTqztOsdmrEJ_Pw</th>\n",
       "      <td>great service ! ! friendly staff ! vegan selec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-2gOxVWcnBr5DclrrsWXCA</th>\n",
       "      <td>excellent service usually peracriptions hand w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9vc7n5Qrc0Wp7_NGufj3w</th>\n",
       "      <td>fan ! several times never suprise rooms top no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-Co-ReNx_lXT1xL_Rr0B2g</th>\n",
       "      <td>previous person posted really said restaurant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-Cwg2o01k7InVnljQmncQA</th>\n",
       "      <td>beware sushi not rice extremely dry something ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text\n",
       "user_id                                                                  \n",
       "-1eORDLXTqztOsdmrEJ_Pw  great service ! ! friendly staff ! vegan selec...\n",
       "-2gOxVWcnBr5DclrrsWXCA  excellent service usually peracriptions hand w...\n",
       "-9vc7n5Qrc0Wp7_NGufj3w  fan ! several times never suprise rooms top no...\n",
       "-Co-ReNx_lXT1xL_Rr0B2g  previous person posted really said restaurant ...\n",
       "-Cwg2o01k7InVnljQmncQA  beware sushi not rice extremely dry something ..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to encode the reviews into feature format so that we can go ahead with modelling. For this task we are using TF-IDF vectorizer to extract the features from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994, 500)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_feature_object = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=500)\n",
    "user_feature = user_feature_object.fit_transform(user_df['text'])\n",
    "user_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(810, 500)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_feature_object = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=500)\n",
    "rest_feature = rest_feature_object.fit_transform(rest_df['text'])\n",
    "rest_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since we are going to use Matrix factorization method for CF, we need to transform into matrix fromat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_vect = user_feature.toarray()\n",
    "P = pd.DataFrame(user_vect, index=user_df.index, columns=user_feature_object.get_feature_names())\n",
    "rest_vect = rest_feature.toarray()\n",
    "Q = pd.DataFrame(rest_vect, index=rest_df.index, columns=rest_feature_object.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>30</th>\n",
       "      <th>5</th>\n",
       "      <th>:</th>\n",
       "      <th>...</th>\n",
       "      <th>working</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1eORDLXTqztOsdmrEJ_Pw</th>\n",
       "      <td>0.554708</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.094624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-2gOxVWcnBr5DclrrsWXCA</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-9vc7n5Qrc0Wp7_NGufj3w</th>\n",
       "      <td>0.540844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-Co-ReNx_lXT1xL_Rr0B2g</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.196141</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.109029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.080636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-Cwg2o01k7InVnljQmncQA</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               !         +         -    1   10    2    3   30  \\\n",
       "user_id                                                                         \n",
       "-1eORDLXTqztOsdmrEJ_Pw  0.554708  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "-2gOxVWcnBr5DclrrsWXCA  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "-9vc7n5Qrc0Wp7_NGufj3w  0.540844  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "-Co-ReNx_lXT1xL_Rr0B2g  0.000000  0.196141  0.043433  0.0  0.0  0.0  0.0  0.0   \n",
       "-Cwg2o01k7InVnljQmncQA  0.000000  0.000000  0.000000  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                          5         :  ...  working  worth     would  wrong  \\\n",
       "user_id                                ...                                    \n",
       "-1eORDLXTqztOsdmrEJ_Pw  0.0  0.000000  ...      0.0    0.0  0.094624    0.0   \n",
       "-2gOxVWcnBr5DclrrsWXCA  0.0  0.000000  ...      0.0    0.0  0.000000    0.0   \n",
       "-9vc7n5Qrc0Wp7_NGufj3w  0.0  0.000000  ...      0.0    0.0  0.000000    0.0   \n",
       "-Co-ReNx_lXT1xL_Rr0B2g  0.0  0.109029  ...      0.0    0.0  0.000000    0.0   \n",
       "-Cwg2o01k7InVnljQmncQA  0.0  0.000000  ...      0.0    0.0  0.000000    0.0   \n",
       "\n",
       "                        year  years  yelp  yes       you     yummy  \n",
       "user_id                                                             \n",
       "-1eORDLXTqztOsdmrEJ_Pw   0.0    0.0   0.0  0.0  0.000000  0.000000  \n",
       "-2gOxVWcnBr5DclrrsWXCA   0.0    0.0   0.0  0.0  0.000000  0.000000  \n",
       "-9vc7n5Qrc0Wp7_NGufj3w   0.0    0.0   0.0  0.0  0.000000  0.000000  \n",
       "-Co-ReNx_lXT1xL_Rr0B2g   0.0    0.0   0.0  0.0  0.066148  0.080636  \n",
       "-Cwg2o01k7InVnljQmncQA   0.0    0.0   0.0  0.0  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 500 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now moving on to the target matrix in which each cell will identify the rating for user and corresponding restaurent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(994, 810)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_matrix = pd.pivot_table(data, values='stars', index=['user_id'], columns=['business_id'])\n",
    "target_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will have to decompose each cell value into product of user and restaurent feature vector. So we will be dealing with cells having some value (ignoring NaNs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_matrix = target_matrix.fillna(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now our problem is a kind of optimization problem so we will use gradient descent to solve this convergence of optimation problem.\n",
    "### As time and space permits we can increase the step size to 100+ and error rate <0.001 for better optimization. Here I am using less step size and high error rate just to check the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_factorization_method(R, P , Q, steps, gamma, lamda):\n",
    "    for step in range(steps):\n",
    "        for i in R.index:\n",
    "            for j in R.columns:\n",
    "                if R.loc[i,j]>0:\n",
    "                    eij=R.loc[i,j]-np.dot(P.loc[i],Q.loc[j])\n",
    "                    P.loc[i]=P.loc[i]+gamma*(eij*Q.loc[j]-lamda*P.loc[i])\n",
    "                    Q.loc[j]=Q.loc[j]+gamma*(eij*P.loc[i]-lamda*Q.loc[j])\n",
    "        error=0\n",
    "        for i in R.index:\n",
    "            for j in R.columns:\n",
    "                if R.loc[i,j]>0:\n",
    "                    error= error + pow(R.loc[i,j]-np.dot(P.loc[i],Q.loc[j]),2)+lamda*(pow(np.linalg.norm(P.loc[i]),2)+pow(np.linalg.norm(Q.loc[j]),2))\n",
    "        if error<0.01:\n",
    "            break\n",
    "        \n",
    "    return P,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 35s, sys: 163 ms, total: 4min 35s\n",
      "Wall time: 4min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "P, Q = matrix_factorization_method(target_matrix, P, Q, 20, 0.01, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it is time to make prediction given a user id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user_id = 'bHXujstlLp-QuNr72Meprw'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the test input data to get the expected format as an input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_text = pd.DataFrame([test_set[test_set['user_id']==input_user_id]['text'].values[0]], columns = ['text'])\n",
    "inp_text['text'] = inp_text['text'].apply(clean_reviews)\n",
    "test_feature = user_feature_object.transform(inp_text['text'])\n",
    "test_P = pd.DataFrame(test_feature.toarray(), index=inp_text.index, \n",
    "                         columns=user_feature_object.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>+</th>\n",
       "      <th>-</th>\n",
       "      <th>1</th>\n",
       "      <th>10</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>30</th>\n",
       "      <th>5</th>\n",
       "      <th>:</th>\n",
       "      <th>...</th>\n",
       "      <th>working</th>\n",
       "      <th>worth</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yelp</th>\n",
       "      <th>yes</th>\n",
       "      <th>you</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.144335</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          !    +    -    1   10    2    3   30    5    :  ...  working  worth  \\\n",
       "0  0.144335  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...      0.0    0.0   \n",
       "\n",
       "   would  wrong  year  years  yelp  yes  you  yummy  \n",
       "0    0.0    0.0   0.0    0.0   0.0  0.0  0.0    0.0  \n",
       "\n",
       "[1 rows x 500 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the prediction based on updated Q matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ratings</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MjOk1rCc0puNfBYWdm2Ocw</th>\n",
       "      <td>0.809280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GX9W1U-wsZPqWTgs1_-wRA</th>\n",
       "      <td>0.572960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wAUUgvSJqKdx6x7Lzy79Og</th>\n",
       "      <td>0.568480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y0pTeRLBftD__abekOFj6g</th>\n",
       "      <td>0.557631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VNpQlfOaX4_vEaYcPC1fJg</th>\n",
       "      <td>0.542337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Ratings\n",
       "business_id                     \n",
       "MjOk1rCc0puNfBYWdm2Ocw  0.809280\n",
       "GX9W1U-wsZPqWTgs1_-wRA  0.572960\n",
       "wAUUgvSJqKdx6x7Lzy79Og  0.568480\n",
       "y0pTeRLBftD__abekOFj6g  0.557631\n",
       "VNpQlfOaX4_vEaYcPC1fJg  0.542337"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=pd.DataFrame(np.dot(test_P.loc[0],Q.T),index=Q.index,columns=['Ratings'])\n",
    "recomd =pd.DataFrame.sort_values(predict,['Ratings'],ascending=[0])[:5]\n",
    "recomd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the updated Q matrix for future predictions on test ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.to_pickle(\"models/Q_matrix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filehandler = open(\"models/tf-idf.obj\",\"wb\")\n",
    "pickle.dump(user_feature_object,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-64a80a0f0dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfilehandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/tf-idf.obj\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muser_ft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilehandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "filehandler = open(\"models/tf-idf.obj\",\"r\")\n",
    "user_ft = pickle.load(filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
